{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归 Linear Regression\n",
    "\n",
    "#### 应用场景\n",
    "- 房价预测\n",
    "- 销售额度预测\n",
    "- 贷款额度预测\n",
    "#### 原理\n",
    "线性回归（Linear Regression）是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方程。\n",
    "\n",
    "- 单变量回归：只有一个自变量\n",
    "- 多元回归：多个自变量\n",
    "通用公式  \n",
    "$$h(w)=w_1x_1+w_2x_2+w_3x_3\\dots+b=w^Tx+b$$\n",
    "其中$w,x$可以理解为矩阵：$w=\\begin{pmatrix}b\\\\w_1\\\\w_2\\end{pmatrix},x=\\begin{pmatrix}1\\\\x_1\\\\x_2\\end{pmatrix}$,$b$为偏置  \n",
    "其实就是$y=kx+b$\n",
    "\n",
    "- 找到特征值与目标值间函数关系，这个关系可以理解为线性模型\n",
    "广义的线性模型：\n",
    "- 线性关系：单特征与目标值的关系呈直线关系，两个特征与目标值呈现平面关系\n",
    "- 非线性关系：不能用一个直线或平面来拟合\n",
    "- ***线性关系一定是线性模型***\n",
    "- ***线性模型不一定是线性关系***\n",
    "\n",
    "我们应该怎样从一大堆数据里求出回归方程呢？ 假定输入数据存放在矩阵 $x$ 中，而回归系数存放在向量 $w$ 中。那么对于给定的数据 $X1$，预测结果将会通过 $Y = X1^T w$给出。现在的问题是，手里有一些 $X$ 和对应的 $y$，怎样才能找到 $w$ 呢？一个常用的方法就是找出使误差最小的 $w$ 。这里的误差是指预测 $y$ 值和真实 $y$ 值之间的差值，使用该误差的简单累加将使得正差值和负差值相互抵消，所以我们采用平方误差（实际上就是我们通常所说的最小二乘法）。\n",
    "\n",
    "### 损失和优化\n",
    "目标:求模型参数  \n",
    "模型参数能够使得预测准确  \n",
    "\n",
    "例如，假设某房屋价格与环境参数存在如下关系：\n",
    "- $真实房价=0.02*中心区域距离+0.04*城市一氧化氮浓度\\dots$  \n",
    "\n",
    "那么现在我们随意指定（猜测）一个关系\n",
    "\n",
    "- $预测房价=0.25*中心区域距离+0.14*城市一氧化氮浓度\\dots$\n",
    "\n",
    "真实值和预测结果是存在一定误差的，衡量误差的函数便是：损失函数\n",
    "\n",
    "损失函数:\n",
    "一般是最小二乘法（又称最小平方法）一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。\n",
    "\n",
    "如何去求模型的 $w$ ,使得损失最小？  \n",
    "线性回归常用优化算法：\n",
    "- 正规方程 Normal Equation （较少使用）\n",
    "  - 可以比喻成一个天才\n",
    "  - 当特征过多过复杂时，求解速度太慢并且得不到结果\n",
    "  - 小数据场景有优势\n",
    "  - 直接求解$w$，$X$为特征值矩阵，$y$为目标值矩阵，直接求的最好结果\n",
    "$$w = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "\n",
    "\n",
    "- 梯度下降 Gradient Descent（较多使用）\n",
    "  - 可以比喻成是勤奋努力的普通人\n",
    "  - 不断地改进、试错直到成功\n",
    "  - $\\alpha$ 为学习速率，需要手动指定，$\\alpha$右边整体表示函数下降的方向\n",
    "  - 面对规模十分庞大的数据集，能找到较好的结果\n",
    "$$\n",
    "w1 := w1 - \\alpha\\frac{{\\partial}cost(w0+w1x1)}{{\\partial}w1}\n",
    "$$\n",
    "$$\n",
    "w0 := w0 - \\alpha\\frac{{\\partial}cost(w0+w1x1)}{{\\partial}w1}\n",
    "$$\n",
    "\n",
    "### API\n",
    "~~~python\n",
    "sklearn.linear_model.LinearRegression\n",
    "~~~\n",
    "- 通过正规方程优化\n",
    "- fit_intercept: 是否计算偏置\n",
    "- LinearRegression.coef_: 回归系数\n",
    "- LinearRegression.intercept_: 偏置\n",
    "\n",
    "~~~python\n",
    "sklearn.linear_model.SGDRegressor\n",
    "~~~\n",
    "- SGDRegressor 类实现了随机梯度下降学习，支持不同的loss函数和正则化惩罚项来拟合线性回归模型\n",
    "- loss:损失类型\n",
    "  - squared_error：普通最小二乘法\n",
    "  - huber\n",
    "  - epsilon_insensitive\n",
    "  - squared_epsilon_insensitive\n",
    "- fit_intercept: 是否计算偏置\n",
    "- learning_rate: 学习率的算法，eta是学习率\n",
    "  - ‘constant’: eta = eta0\n",
    "  - ‘optimal’: eta = 1.0 / (alpha * (t + t0))\n",
    "  - ‘invscaling’: eta = eta0 / pow(t, power_t) 逐步减小学习率\n",
    "    - power_t：默认是0.25\n",
    "  - ‘adaptive’: eta = eta0\n",
    "- SGDRegressor.coef_: 回归系数\n",
    "- SGDRegressor.intercept_: 偏置\n",
    "\n",
    "### 梯度下降方法\n",
    "- 梯度下降（Gradient Descent）\n",
    "  - 原始的梯度下降法需要计算所有样本的值才能够得出梯度，计算量大。\n",
    "- 随机梯度下降（SGD，Stochastic gradient descent）\n",
    "  - 每次迭代时只考虑一个训练样本\n",
    "  - 优点是：高效，容易实现\n",
    "  - 缺点是：需要设置许多超参数，如正则项参数，迭代数，SGD对特征标准化敏感\n",
    "- 随机平均梯度法（SAG，Stochastic Average Gradient）\n",
    "  - 岭回归、逻辑回归中有SAG优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boston house price\n",
    "# length: 506\n",
    "# CRIM - 城镇人均犯罪率                                                              ------【城镇人均犯罪率】\n",
    "# ZN - 占地面积超过25,000平方英尺的住宅用地比例。               ------【住宅用地所占比例】\n",
    "# INDUS - 每个城镇非零售业务的比例。                                      ------【城镇中非商业用地占比例】\n",
    "# CHAS - Charles River虚拟变量（如果是河道，则为1;否则为0  ------【查尔斯河虚拟变量，用于回归分析】\n",
    "# NOX - 一氧化氮浓度（每千万份）                                             ------【环保指标】\n",
    "# RM - 每间住宅的平均房间数                                                      ------【每栋住宅房间数】\n",
    "# AGE - 1940年以前建造的自住单位比例                                     ------【1940年以前建造的自住单位比例 】\n",
    "# DIS -波士顿的五个就业中心加权距离                                        ------【与波士顿的五个就业中心加权距离】\n",
    "# RAD - 径向高速公路的可达性指数                                             ------【距离高速公路的便利指数】\n",
    "# TAX - 每10,000美元的全额物业税率                                          ------【每一万美元的不动产税率】\n",
    "# PTRATIO - 城镇的学生与教师比例                                             ------【城镇中教师学生比例】\n",
    "# B - 1000（Bk - 0.63）^ 2其中Bk是城镇黑人的比例                   ------【城镇中黑人比例】\n",
    "# LSTAT - 人口状况下降％                                                            ------【房东属于低等收入阶层比例】\n",
    "# MEDV - 自有住房的中位数报价, 单位1000美元                         ------【自住房屋房价中位数】\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\source\\tools\\miniconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取数据集\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "boston.data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.80028e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        2.27610e+02, 1.21400e+01],\n",
       "       [2.14090e-01, 2.20000e+01, 5.86000e+00, ..., 1.91000e+01,\n",
       "        3.77070e+02, 3.59000e+00],\n",
       "       [2.99160e-01, 2.00000e+01, 6.96000e+00, ..., 1.86000e+01,\n",
       "        3.88650e+02, 1.30000e+01],\n",
       "       ...,\n",
       "       [4.41700e-02, 7.00000e+01, 2.24000e+00, ..., 1.48000e+01,\n",
       "        3.90860e+02, 6.07000e+00],\n",
       "       [5.90050e-01, 0.00000e+00, 2.18900e+01, ..., 2.12000e+01,\n",
       "        3.85760e+02, 1.11200e+01],\n",
       "       [1.11081e+01, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        3.96900e+02, 3.47700e+01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 划分数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(boston.data,boston.target,random_state=22)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20918739, -0.52054534,  1.24477931, ..., -1.72807526,\n",
       "        -1.35897227, -0.08447646],\n",
       "       [-0.43088174,  0.3974553 , -0.7606374 , ...,  0.30003249,\n",
       "         0.24261973, -1.25661718],\n",
       "       [-0.41899191,  0.3140007 , -0.59985326, ...,  0.0695657 ,\n",
       "         0.36670935,  0.03342307],\n",
       "       ...,\n",
       "       [-0.45463067,  2.4003658 , -1.28976339, ..., -1.6819819 ,\n",
       "         0.3903914 , -0.91662782],\n",
       "       [-0.37833557, -0.52054534,  1.582426  , ...,  1.26799301,\n",
       "         0.33574053, -0.22431079],\n",
       "       [ 1.09172307, -0.52054534,  1.02845156, ...,  0.80705943,\n",
       "         0.45511517,  3.0179264 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.transform(x_test)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line coef_: [-0.64817766  1.14673408 -0.05949444  0.74216553 -1.95515269  2.70902585\n",
      " -0.07737374 -3.29889391  2.50267196 -1.85679269 -1.75044624  0.87341624\n",
      " -3.91336869]\n",
      "line intercept_: 22.62137203166228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([28.22944896, 31.5122308 , 21.11612841, 32.6663189 , 20.0023467 ,\n",
       "       19.07315705, 21.09772798, 19.61400153, 19.61907059, 32.87611987,\n",
       "       20.97911561, 27.52898011, 15.54701758, 19.78630176, 36.88641203,\n",
       "       18.81202132,  9.35912225, 18.49452615, 30.66499315, 24.30184448,\n",
       "       19.08220837, 34.11391208, 29.81386585, 17.51775647, 34.91026707,\n",
       "       26.54967053, 34.71035391, 27.4268996 , 19.09095832, 14.92742976,\n",
       "       30.86877936, 15.88271775, 37.17548808,  7.72101675, 16.24074861,\n",
       "       17.19211608,  7.42140081, 20.0098852 , 40.58481466, 28.93190595,\n",
       "       25.25404307, 17.74970308, 38.76446932,  6.87996052, 21.80450956,\n",
       "       25.29110265, 20.427491  , 20.4698034 , 17.25330064, 26.12442519,\n",
       "        8.48268143, 27.50871869, 30.58284841, 16.56039764,  9.38919181,\n",
       "       35.54434377, 32.29801978, 21.81298945, 17.60263689, 22.0804256 ,\n",
       "       23.49262401, 24.10617033, 20.1346492 , 38.5268066 , 24.58319594,\n",
       "       19.78072415, 13.93429891,  6.75507808, 42.03759064, 21.9215625 ,\n",
       "       16.91352899, 22.58327744, 40.76440704, 21.3998946 , 36.89912238,\n",
       "       27.19273661, 20.97945544, 20.37925063, 25.3536439 , 22.18729123,\n",
       "       31.13342301, 20.39451125, 23.99224334, 31.54729547, 26.74581308,\n",
       "       20.90199941, 29.08225233, 21.98331503, 26.29101202, 20.17329401,\n",
       "       25.49225305, 24.09171045, 19.90739221, 16.35154974, 15.25184758,\n",
       "       18.40766132, 24.83797801, 16.61703662, 20.89470344, 26.70854061,\n",
       "       20.7591883 , 17.88403312, 24.28656105, 23.37651493, 21.64202047,\n",
       "       36.81476219, 15.86570054, 21.42338732, 32.81366203, 33.74086414,\n",
       "       20.61688336, 26.88191023, 22.65739323, 17.35731771, 21.67699248,\n",
       "       21.65034728, 27.66728556, 25.04691687, 23.73976625, 14.6649641 ,\n",
       "       15.17700342,  3.81620663, 29.18194848, 20.68544417, 22.32934783,\n",
       "       28.01568563, 28.58237108])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预估器\n",
    "\n",
    "# 使用正规方程优化\n",
    "from sklearn.linear_model import LinearRegression\n",
    "line = LinearRegression()\n",
    "line.fit(x_train, y_train)\n",
    "print(\"line coef_:\",line.coef_)\n",
    "print(\"line intercept_:\",line.intercept_)\n",
    "\n",
    "line_pred = line.predict(x_test)\n",
    "line_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd coef_: [-0.47164946  0.84810581 -0.4221744   0.82214427 -1.56692709  2.87175833\n",
      " -0.14959994 -2.99664564  1.55990926 -0.83326476 -1.68637887  0.89296662\n",
      " -3.87746099]\n",
      "sgd intercept_: [22.64276849]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([28.35283663, 31.54637077, 21.47491967, 32.75646174, 20.3418425 ,\n",
       "       19.29446058, 21.36624875, 19.47855402, 19.73561328, 32.82279733,\n",
       "       21.3507788 , 27.36004763, 15.68800001, 20.09589517, 37.10427405,\n",
       "       18.5428127 ,  9.87382737, 18.69597346, 30.81744182, 24.26600453,\n",
       "       19.25396239, 34.03095316, 29.59424714, 17.55575506, 34.74764334,\n",
       "       26.3510561 , 34.10873088, 27.3125338 , 19.33301543, 15.91182652,\n",
       "       30.76065113, 14.40884207, 37.33781468,  9.2308758 , 16.48122096,\n",
       "       16.88240854,  8.07755489, 19.91315668, 40.53363643, 29.24633927,\n",
       "       25.27684054, 18.03776184, 39.72276732,  6.84276548, 21.63314443,\n",
       "       24.94440225, 21.15763781, 20.80655367, 17.07290349, 26.57373272,\n",
       "        9.85179393, 27.03746744, 30.735683  , 16.95776741,  9.74698534,\n",
       "       35.3966156 , 31.21061938, 23.02185615, 17.62052505, 21.88401492,\n",
       "       23.59321351, 23.85375653, 20.44220105, 38.06950276, 25.77576132,\n",
       "       19.83708875, 14.35199226,  6.88975798, 42.71285914, 21.80161826,\n",
       "       16.67988946, 22.69313491, 40.93119892, 21.7299462 , 36.85451569,\n",
       "       27.09958121, 21.9696067 , 20.66522131, 25.30326909, 23.98437737,\n",
       "       31.48015829, 20.14582884, 24.08172119, 31.29247753, 27.27489309,\n",
       "       21.0640764 , 28.96324993, 22.0956306 , 26.77045339, 18.71745122,\n",
       "       24.9369942 , 24.00272006, 20.14858025, 18.48025152, 15.64348306,\n",
       "       18.39985158, 24.43963907, 16.88742937, 20.77596319, 26.82955575,\n",
       "       20.88874423, 18.23907825, 24.13761354, 23.20635653, 20.23238631,\n",
       "       36.47154075, 16.09443107, 22.50069316, 32.55487634, 33.86744123,\n",
       "       20.55893897, 25.79844249, 23.66239231, 17.76662967, 21.48394116,\n",
       "       21.67168931, 27.40207966, 25.17731165, 23.62373314, 14.57241312,\n",
       "       15.89186979,  3.78573132, 29.14590199, 20.86584016, 22.23470928,\n",
       "       28.00504312, 28.31255636])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用梯度下降优化\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(x_train, y_train)\n",
    "print(\"sgd coef_:\",sgd.coef_)\n",
    "print(\"sgd intercept_:\",sgd.intercept_)\n",
    "\n",
    "sgd_pred = sgd.predict(x_test)\n",
    "sgd_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params_ {'eta0': 0.01, 'learning_rate': 'constant', 'max_iter': 50000, 'penalty': 'l1'}\n",
      "best_score_ 0.7004690762216201\n",
      "sgdGS coef_: [-0.00602056  1.38560729 -0.03802939  1.54275532 -2.02685416  3.35396475\n",
      " -0.20073693 -3.25456587  2.7293583  -1.54110265 -1.86376158  0.57077494\n",
      " -4.31436139]\n",
      "sgdGS intercept_: [22.83663945]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([27.97880973, 31.58264544, 20.13298324, 36.12434121, 19.19334954,\n",
       "       19.46754043, 19.94108356, 18.38035793, 18.68311012, 33.51539277,\n",
       "       19.90465573, 28.99797756, 13.86494221, 18.93850598, 38.54000505,\n",
       "       17.24303266,  7.04890975, 17.34242382, 31.07526375, 24.11196777,\n",
       "       19.37889203, 34.5752468 , 32.57044789, 18.60302102, 35.41909387,\n",
       "       25.56743415, 36.06208055, 27.98298053, 20.31165987, 14.06562168,\n",
       "       30.91927924, 13.58415368, 39.44333995, 14.83309218, 14.79647148,\n",
       "       17.18057434,  9.11638119, 20.44893909, 45.39067719, 29.24705634,\n",
       "       24.87823977, 18.75606998, 45.40327997,  5.9671942 , 22.29149707,\n",
       "       24.64158046, 22.23521007, 19.7341643 , 18.25633081, 28.80643758,\n",
       "        6.91116548, 27.24494423, 33.85481775, 18.19380296, 11.11067092,\n",
       "       36.07062765, 33.12803101, 21.66148332, 16.07814946, 21.98615985,\n",
       "       23.03680447, 23.03639171, 19.17959546, 39.91764422, 24.73385947,\n",
       "       20.07262826, 15.60981783,  6.58428582, 47.45149148, 21.29045387,\n",
       "       18.89671816, 22.13385164, 42.51374802, 20.92975183, 37.77073555,\n",
       "       26.6653867 , 21.2398456 , 20.77905601, 25.04390344, 22.15944221,\n",
       "       31.33514544, 19.10577509, 26.32929688, 32.98583602, 26.67384019,\n",
       "       21.71149805, 28.56924264, 21.46761163, 26.19960764, 18.37274877,\n",
       "       25.57955798, 26.16046127, 20.6073654 , 24.76363445, 17.47873703,\n",
       "       18.61428092, 23.66091708, 18.64492554, 21.28060029, 26.27216209,\n",
       "       21.65461826, 18.52407302, 23.56283986, 22.2370065 , 20.02040763,\n",
       "       40.87410611, 14.36946979, 20.94150616, 33.94782808, 38.65081665,\n",
       "       19.27456274, 26.35357413, 25.3914847 , 15.99017106, 20.61208594,\n",
       "       21.85299585, 28.64388262, 25.83151568, 22.7230115 , 14.13249098,\n",
       "       19.71629897,  3.26685171, 28.78412874, 21.07163732, 21.23401989,\n",
       "       27.79754964, 28.3432459 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 网格搜索对梯度下降调参\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "estimator = SGDRegressor()\n",
    "params = {\n",
    "    \"penalty\":[\"l2\",\"l1\"],\n",
    "    \"learning_rate\":[\"constant\",\"optimal\",\"invscaling\",\"adaptive\"],\n",
    "    \"eta0\":[0.1,0.05,0.01,0.005,0.001],\n",
    "    \"max_iter\":[500,800,1000,5000,10000,20000,50000]\n",
    "}\n",
    "grid = GridSearchCV(estimator,params,cv=3,n_jobs=-1)\n",
    "grid.fit(x_train, y_train)\n",
    "print(\"best_params_\",grid.best_params_)\n",
    "print(\"best_score_\",grid.best_score_)\n",
    "\n",
    "\n",
    "sgdGS = SGDRegressor(eta0=grid.best_params_[\"eta0\"],learning_rate=grid.best_params_[\"learning_rate\"],penalty=grid.best_params_[\"penalty\"],max_iter=grid.best_params_[\"max_iter\"])\n",
    "sgdGS.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(\"sgdGS coef_:\",sgdGS.coef_)\n",
    "print(\"sgdGS intercept_:\",sgdGS.intercept_)\n",
    "\n",
    "\n",
    "sgdGS_pred = sgdGS.predict(x_test)\n",
    "sgdGS_pred"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "455d156c2443f23ffeb7a3cb87ade32cade494afff05783b4faa4f6a104c976b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
